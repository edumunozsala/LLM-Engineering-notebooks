{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1W499zNqDRtbXD6wCzeKCGLqPM_DsDyEk","timestamp":1699406444622},{"file_id":"1FgUKk5TNcwTe2J4GJdTF04nVKFXKFlP5","timestamp":1699400865719}],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Unsupervised Pre-Training of GPT-Style Model\n","\n","In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n","\n","The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n","\n","All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n","\n","> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."],"metadata":{"id":"UWiGVj6njoDn"}},{"cell_type":"markdown","source":["## Data Selection\n","\n","For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n","\n","You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n","\n","> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n","\n","Let's start by grabbing our source repository for the day!"],"metadata":{"id":"eHi04aEnkKEZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMRsEQZy6tgc","executionInfo":{"status":"ok","timestamp":1699468025239,"user_tz":-60,"elapsed":1220,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"89afabe6-0255-47c6-fa5e-b79f1d12617c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'nanoGPT'...\n","remote: Enumerating objects: 649, done.\u001b[K\n","remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n","Receiving objects: 100% (649/649), 936.46 KiB | 44.59 MiB/s, done.\n","Resolving deltas: 100% (371/371), done.\n"]}],"source":["!git clone https://github.com/karpathy/nanoGPT.git"]},{"cell_type":"markdown","source":["Next, we'll need to grab some dependencies.\n","\n","`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."],"metadata":{"id":"6l4CqoEDl7ks"}},{"cell_type":"code","source":["!pip install tiktoken requests cohere openai -qU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_gepPv1Qdj_","executionInfo":{"status":"ok","timestamp":1699468038220,"user_tz":-60,"elapsed":7858,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"ac112e38-c8e4-4ff9-8009-e3634c1c78c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.9/217.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["First things first - let's download our dataset!\n","\n","We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set. We want ~90% of our data to be training, and ~10% to be validation."],"metadata":{"id":"70hSjXmZmCt3"}},{"cell_type":"code","source":["import os\n","import requests\n","import tiktoken\n","import numpy as np\n","\n","current_path = \"/data/shakespeare\"\n","data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","\n","if not os.path.exists(current_path):\n","    os.makedirs(current_path)\n","\n","# download the tiny shakespeare dataset\n","input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n","if not os.path.exists(input_file_path):\n","\n","    with open(input_file_path, 'w') as f:\n","        f.write(requests.get(data_url).text)\n","\n","with open(input_file_path, 'r') as f:\n","    data = f.read()\n","\n","n = len(data)\n","train_data = data[:int(0.9*n)]\n","print('Length Train data: ',len(train_data))\n","val_data = data[int(0.9*n):]### YOUR CODE HERE\n","print('Length Val data: ',len(val_data))"],"metadata":{"id":"T7qRWArUNiZ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699468057194,"user_tz":-60,"elapsed":989,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"033737f0-5bee-4271-f045-a76091d78b12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length Train data:  1003854\n","Length Val data:  111540\n"]}]},{"cell_type":"markdown","source":["Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."],"metadata":{"id":"wU9BG2CymU-a"}},{"cell_type":"code","source":["!pip install tokenizers -qU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFnrwKpQPsYh","executionInfo":{"status":"ok","timestamp":1699468066916,"user_tz":-60,"elapsed":5761,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"e5deae1d-70ff-4d00-817e-61515e526246"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n","\n","Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n","\n","\n","\n"],"metadata":{"id":"rmWXE5ctma9Z"}},{"cell_type":"markdown","source":["### What is BPE?\n","\n","First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n","\n","The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n","\n","Let's take the following text and break it apart into its word components.\n","\n","\n","```\n","After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n","```\n","\n","A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."],"metadata":{"id":"GLecDiHbogvX"}},{"cell_type":"code","source":["input_text = \"\"\"\n","After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n","\"\"\"\n","\n","naive_word_list = input_text.split()"],"metadata":{"id":"m34NDAGCpiz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can count our words and get their frequency."],"metadata":{"id":"hR8k-2bopqjy"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","vocab_and_frequencies = defaultdict(int)\n","\n","for word in naive_word_list:\n","  vocab_and_frequencies[\" \".join(list(word))] += 1\n","\n","sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_201bSQpvqD","executionInfo":{"status":"ok","timestamp":1699461779692,"user_tz":-60,"elapsed":659,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"d1c23115-c992-44d0-a5de-bff75cf2acbb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."],"metadata":{"id":"NckufSxxp-w5"}},{"cell_type":"code","source":["from typing import Dict, Tuple, List, Set\n","\n","def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n","  vocab = set()\n","\n","  for word in current_vocab.keys():\n","    for subword in word.split():\n","      vocab.add(subword)\n","\n","  return len(vocab)"],"metadata":{"id":"BNcjzjDvvKjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["find_vocabulary_size(vocab_and_frequencies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pf3kCf-WvdBL","executionInfo":{"status":"ok","timestamp":1699461809491,"user_tz":-60,"elapsed":629,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"5a9ebf80-564c-46e2-b767-135a0ab23639"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["As we can see, there are ~34 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."],"metadata":{"id":"VoMq7GhKqf7p"}},{"cell_type":"markdown","source":["Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."],"metadata":{"id":"OGxrHYmftDTr"}},{"cell_type":"code","source":["def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  pairs = {}\n","\n","  for word, frequency in current_vocab.items():\n","    symbols = word.split()\n","\n","    for i in range(len(symbols) - 1):\n","      pair = (symbols[i], symbols[i + 1])\n","      current_frequency = pairs.get(pair, 0)\n","      pairs[pair] = current_frequency + frequency\n","\n","  return pairs"],"metadata":{"id":"sTwvfTAErQN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"],"metadata":{"id":"FudOaKmYv9-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGIJfkk7wFYw","executionInfo":{"status":"ok","timestamp":1699461927662,"user_tz":-60,"elapsed":10,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"63da6a1c-8692-457b-feb9-5c3e93941610"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('t', 'h'), 11),\n"," (('i', 'n'), 10),\n"," (('r', 'e'), 8),\n"," (('h', 'e'), 8),\n"," (('a', 't'), 7)]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Now that we have the frequent pairs - we can merge those pairs into a single token.\n","\n","Let's see how this process looks in code."],"metadata":{"id":"OqORqdzwsZ6s"}},{"cell_type":"code","source":["import re\n","\n","def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  vocab_out = {}\n","\n","  pattern = re.escape(' '.join(most_common_pair))\n","  replacement = ''.join(most_common_pair)\n","\n","  for word_in in current_vocab:\n","      word_out = re.sub(pattern, replacement, word_in)\n","      vocab_out[word_out] = current_vocab[word_in]\n","\n","  return vocab_out"],"metadata":{"id":"L7ohHm2kshoY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_vocab_and_frequencies = merge_vocab(\n","    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n","    vocab_and_frequencies\n",")"],"metadata":{"id":"Ab760KKuwzZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0XtvLbpxbSx","executionInfo":{"status":"ok","timestamp":1699461951916,"user_tz":-60,"elapsed":472,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"3fa0ffe2-bee7-48d1-b65b-acdf3a3f6f67"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["After one merge, we can see that `t h` has been converted to `th`!\n","\n","Let's see how that impacted our vocabulary."],"metadata":{"id":"9DPkBzj2u-me"}},{"cell_type":"code","source":["find_vocabulary_size(new_vocab_and_frequencies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bO_xegCtxjQf","executionInfo":{"status":"ok","timestamp":1699461954881,"user_tz":-60,"elapsed":3,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"672fce9f-e2d5-41fc-ab0e-8bbde6324044"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n","\n","In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"],"metadata":{"id":"o3M13D60xzZi"}},{"cell_type":"markdown","source":["## Training Our Tokenizer\n","\n","Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n","\n","Let's walk through the steps we'll take:\n","\n","1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n","\n","  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n","  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n","\n","2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n","\n","  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n","\n","3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n","\n","  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n","  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"],"metadata":{"id":"BePYCbHly02H"}},{"cell_type":"code","source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from tokenizers.normalizers import NFD, Sequence\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import ByteLevel\n","\n","# Create the tokenizer\n","tokenizer = Tokenizer(BPE(unk_token= '[UNK]')) ### YOUR CODE HERE\n","tokenizer.normalizer = Sequence([NFD()])\n","tokenizer.pre_tokenizer = ByteLevel()### YOUR CODE HERE\n","tokenizer.decoder = ByteLevelDecoder()### YOUR CODE HERE"],"metadata":{"id":"OrztE09OPosB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the normalizer and pre-tokenizer:"],"metadata":{"id":"r5FCCyZH1mnF"}},{"cell_type":"code","source":["# Normalize a text sample\n","print(tokenizer.normalizer.normalize_str(\"Hark, my name be Romeo! I am but a beautiful summer's day!\"))\n","# Pre-tokenize a text sample\n","tokenizer.pre_tokenizer.pre_tokenize_str(\"Hark, my name be Romeo! I am but a beautiful summer's day!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77YdB-Pz1rTm","executionInfo":{"status":"ok","timestamp":1699468086782,"user_tz":-60,"elapsed":11,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"ca805d10-e7d7-4dc4-8bdb-ba938acac8a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('ĠHark', (0, 4)),\n"," (',', (4, 5)),\n"," ('Ġmy', (5, 8)),\n"," ('Ġname', (8, 13)),\n"," ('Ġbe', (13, 16)),\n"," ('ĠRomeo', (16, 22)),\n"," ('!', (22, 23)),\n"," ('ĠI', (23, 25)),\n"," ('Ġam', (25, 28)),\n"," ('Ġbut', (28, 32)),\n"," ('Ġa', (32, 34)),\n"," ('Ġbeautiful', (34, 44)),\n"," ('Ġsummer', (44, 51)),\n"," (\"'s\", (51, 53)),\n"," ('Ġday', (53, 57)),\n"," ('!', (57, 58))]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n","\n","Let's use the following:\n","\n","- `\"<s>\"`    : bos_token - beginning of sequence token\n","- `\"</s>\"`   : eos_token - end of sequence token\n","- `\"<pad>\"`  : padding_token - token used to pad sequences\n","- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n","- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n","\n","We're also going to set a target vocabulary of 50,000 tokens."],"metadata":{"id":"dDqkNNdM1KsD"}},{"cell_type":"code","source":["# Define the trainer the tokenizer\n","trainer = BpeTrainer(\n","    vocab_size= 50000, ### YOUR CODE HERE,\n","    show_progress=True,\n","    special_tokens=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"<mask>\"]\n",")"],"metadata":{"id":"x9iQVhN3P3RN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nothing left to do but point it at our data-source and let it train!\n","\n","We'll use the `.train()` method to accomplish this task.\n","\n","> NOTE: Pay attention to the desired inputs of the `.train()` method.\n","\n","- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"],"metadata":{"id":"yQ8X9vZe2Fyw"}},{"cell_type":"code","source":["# Train the tokenizer\n","tokenizer.train(files=[input_file_path], trainer=trainer\n",")"],"metadata":{"id":"LinLHotSP7gv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"],"metadata":{"id":"V2JNYiqB2qKV"}},{"cell_type":"code","source":["save_path = '/content/tokenizer'\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","tokenizer.model.save(save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jk6QjDGHQy2K","executionInfo":{"status":"ok","timestamp":1699468103283,"user_tz":-60,"elapsed":465,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"020018c5-e1e3-4de6-8bfe-5e866b5e2226"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["!pip install transformers -qU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cOOlbggdRFrN","executionInfo":{"status":"ok","timestamp":1699468115143,"user_tz":-60,"elapsed":10131,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"0d05bacc-01d7-447b-8575-2e6ee596c4dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"],"metadata":{"id":"us1vofdhQ45C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how it tokenizes our inputs!"],"metadata":{"id":"0-Bnq7lV2xWo"}},{"cell_type":"code","source":["input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\"\n","tokenizer.encode(input_sentence)"],"metadata":{"id":"dnYnFa3fTRLf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699468129384,"user_tz":-60,"elapsed":642,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"08ce16d9-e574-4476-dc25-29a2965fc688"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["tokenized_sentence = tokenizer.tokenize(input_sentence) ### YOUR CODE HERE\n","tokenized_sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSHY5VufRbBj","executionInfo":{"status":"ok","timestamp":1699468146861,"user_tz":-60,"elapsed":462,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"58b35fff-00fd-406b-c2c2-79acfba4554d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hark',\n"," ',',\n"," 'Ġmy',\n"," 'Ġname',\n"," 'Ġbe',\n"," 'ĠRomeo',\n"," '!',\n"," 'ĠI',\n"," 'Ġam',\n"," 'Ġbut',\n"," 'Ġa',\n"," 'Ġbeautiful',\n"," 'Ġsummer',\n"," \"'s\",\n"," 'Ġday',\n"," '!']"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["encoded_tokens = tokenizer.encode(input_sentence) ### YOUR CODE HERE\n","encoded_tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZrWzQQlTU41","executionInfo":{"status":"ok","timestamp":1699468163805,"user_tz":-60,"elapsed":1052,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"06194bf0-96aa-4b62-dad8-ed5b70732896"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["decoded_tokens = tokenizer.decode(encoded_tokens)### YOUR CODE HERE\n","decoded_tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"oS6lE-NLRnzk","executionInfo":{"status":"ok","timestamp":1699468178462,"user_tz":-60,"elapsed":2810,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"95a212b1-321e-4431-ccd3-814679c8b95c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Hark, my name be Romeo! I am but a beautiful summer's day!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Tokenizing Dataset\n","\n","Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n","\n","We'll simply encode our training and validation data - and then save them in binary files for later!\n","\n","> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."],"metadata":{"id":"ji3sF-rA21YH"}},{"cell_type":"code","source":["train_ids = tokenizer.encode(train_data)### YOUR CODE HERE\n","val_ids = tokenizer.encode(val_data) ### YOUR CODE HERE\n","print(f\"train has {len(train_ids):,} tokens\")\n","print(f\"val has {len(val_ids):,} tokens\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"calHML6JPnCU","executionInfo":{"status":"ok","timestamp":1699468203684,"user_tz":-60,"elapsed":2321,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"972fd885-ad65-4f15-ba8a-69497ad6a02c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train has 291,284 tokens\n","val has 34,223 tokens\n"]}]},{"cell_type":"code","source":["# export to bin files\n","data_path = \"/data/shakespeare/\"\n","\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n","val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"],"metadata":{"id":"nKJ1KqiiPkRh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training The Model\n","\n","Now that we have our tokenized dataset, let's get to training our model!\n","\n","We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n","\n","First, let's literally jump into the `nanoGPT` repository we cloned earlier."],"metadata":{"id":"c0I3VrRC3XIO"}},{"cell_type":"code","source":["%cd nanoGPT"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUU2jaalUdqm","executionInfo":{"status":"ok","timestamp":1699468209693,"user_tz":-60,"elapsed":6,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"ba604175-e53d-4f51-91a0-eb7709b48697"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/nanoGPT\n"]}]},{"cell_type":"markdown","source":["We'll do some critical imports."],"metadata":{"id":"13p1e8sa3k0V"}},{"cell_type":"code","source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","\n","# from the local repo\n","from model import GPTConfig, GPT"],"metadata":{"id":"weNR37BwUYNg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hyper-Parameters\n","\n","We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."],"metadata":{"id":"kY_vWZG-3uM-"}},{"cell_type":"markdown","source":["#### I/O\n","\n","- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"],"metadata":{"id":"OykCjVQK5EX-"}},{"cell_type":"code","source":["out_dir = 'out'"],"metadata":{"id":"viM3qlWt5PVS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Initialization\n","\n","Since we're training from scratch, we'll use `init_from = 'scratch'`."],"metadata":{"id":"A5iwwrNL5H4C"}},{"cell_type":"code","source":["init_from = 'scratch'"],"metadata":{"id":"OK1z2m3C312T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Eval and Logging\n","\n","- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n","- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n","- `eval_iters` - this is how *many* iterations we want to evaluate for.\n","- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n","- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."],"metadata":{"id":"2YlolKOj4_dE"}},{"cell_type":"code","source":["eval_interval = 250 ### YOUR CODE HERE\n","eval_iters = 200 ### YOUR CODE HERE\n","log_interval = 10 ### YOUR CODE HERE\n","eval_only = False\n","always_save_checkpoint = True"],"metadata":{"id":"MbFN5Ltq4_mo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Dataset\n","\n","We can set our dataset here - we'll use the one we created earlier!"],"metadata":{"id":"a488zaF_4zQk"}},{"cell_type":"code","source":["dataset = 'shakespeare'"],"metadata":{"id":"_QC7vWXC40Hp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Typical Hyper-Parameters\n","\n","- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n","- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n","- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."],"metadata":{"id":"XP9rBgGc426Q"}},{"cell_type":"code","source":["gradient_accumulation_steps =1 ### YOUR CODE HERE\n","batch_size =16 ### YOUR CODE HERE\n","block_size =512 ### YOUR CODE HERE"],"metadata":{"id":"EM_ybLPP43Pd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Model Architecture\n","\n","- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n","- `n_head` - this is the number of attention heads in each decoder layer!\n","- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook. A default value of ~`500` should do the trick!\n","- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n","- `bias` - wether or not to use bias inside the LayerNorm/Linear layers."],"metadata":{"id":"UZ-8bDIY45GS"}},{"cell_type":"code","source":["n_layer = 6 ### YOUR CODE HERE\n","n_head = 6 ### YOUR CODE HERE\n","n_embd = 516 ### YOUR CODE HERE\n","dropout = 0.2\n","bias = False"],"metadata":{"id":"gMyyDBxB6k4H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ❓ QUESTION:\n","\n","What condition must be true as it relates to the `n_embd` and `n_head`?\n","\n","*n_embd % n_head ==0*"],"metadata":{"id":"piNHkSaRNDjM"}},{"cell_type":"markdown","source":["#### Optimizer Hyper-Parameters\n","\n","Basic Optimizer Hyper-Parameters:\n","\n","- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n","- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n","\n","Learning Rate Decay Settings:\n","\n","- `decay_lr` - set decay flag\n","- `weight_Decay` - how much to decay lr by\n","- `lr_decay_iters` - should be set to ~max_iters.\n","- `min_lr` - the minimum lr, should be ~ lr / 10\n","\n","Clipping and Warmup:\n","\n","- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n","- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n","\n","> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."],"metadata":{"id":"3NWDTaAz7gwh"}},{"cell_type":"code","source":["# adamw optimizer\n","learning_rate = 1e-3 ### YOUR CODE HERE\n","max_iters = 5000 ### YOUR CODE HERE\n","beta1 = 0.9\n","beta2 = 0.99\n","\n","# lr decay settings\n","decay_lr = True\n","weight_decay = 1e-1 ### YOUR CODE HERE\n","lr_decay_iters = 5000 # ~= max_iters per Chinchilla\n","min_lr = learning_rate/10 # ~= learning_rate/10 per Chinchilla\n","\n","# clipping and warmup\n","grad_clip = 1.0\n","warmup_iters = 100"],"metadata":{"id":"qe-669jwUptI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["1e-04 / 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuZUV3XV14_L","executionInfo":{"status":"ok","timestamp":1699546322203,"user_tz":-60,"elapsed":11,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"e32148c7-f8a7-4252-c030-c7c02296a94e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1e-05"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["#### ❓ QUESTION:\n","\n","Given a Learning Rate of `1e-4` and a maximum iteration cap of `10,000`: What should `lr_decay_iters` be, and what should `min_lr` be?\n","\n","*`lr_decay_iters` should be set to maximum iterations (max_iters=10,000) and `min_lr` should be `1-e05`.*\n"],"metadata":{"id":"AzHvpMDTNfU6"}},{"cell_type":"markdown","source":["These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."],"metadata":{"id":"ucldc4mz9yeT"}},{"cell_type":"code","source":["backend = 'nccl'\n","device = 'cuda'\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n","compile = True\n","# -----------------------------------------------------------------------------\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys}\n","# -----------------------------------------------------------------------------\n","master_process = True\n","seed_offset = 0\n","ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n","os.makedirs(out_dir, exist_ok=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHiGlMOp8Nux","executionInfo":{"status":"ok","timestamp":1699468246662,"user_tz":-60,"elapsed":475,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"ec207afa-17d1-4c78-94e1-cace35eed4f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tokens per iteration will be: 8,192\n"]}]},{"cell_type":"markdown","source":["### Torch Settings\n","\n","We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n","\n","Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."],"metadata":{"id":"eKmdfbye-BNf"}},{"cell_type":"code","source":["torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu'\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"],"metadata":{"id":"yh34QGD6VARU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataloader\n","\n","This block will:\n","\n","1. Set the data path\n","2. Load the dataset we tokenized earlier from the `.bin` we saved\n","3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."],"metadata":{"id":"gKeNwYaZ-Zoc"}},{"cell_type":"code","source":["data_dir = os.path.join('/data', dataset)\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y"],"metadata":{"id":"tOjaPyJpVEgx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ❓ Question:\n","\n","What can you tell us about the way the labels are generated?\n","\n","Please produce an example of a single x and y pair.\n","\n","*Labels must be shifted +1 position to the left in the input string.*\n","\n","*Example:*\n","\n","x=[\"The\", \"boy\", \"was\", \"sitting\", \"in\", \"the\", \"chair\"]\n","y=[\"boy\", \"was\", \"sitting\", \"in\", \"the\", \"chair\", \"and\"]"],"metadata":{"id":"I-tifZVD-9hG"}},{"cell_type":"markdown","source":["### Simple Initialization of Model\n","\n","Here we init our number of iterations as 0, and our best val loss as a very high number."],"metadata":{"id":"EbDlW-68_atH"}},{"cell_type":"code","source":["iter_num = 0\n","best_val_loss = 1e9"],"metadata":{"id":"6hsepdVBVzQU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Obtain our vocab size from our trained tokenizer."],"metadata":{"id":"A4Uj9qBI_vXc"}},{"cell_type":"code","source":["data_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"GlIXgD4A7tXk","executionInfo":{"status":"ok","timestamp":1699468277318,"user_tz":-60,"elapsed":6,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"de96c2f5-8938-4a43-acd2-eef0a1c3fba0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/data/shakespeare'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = tokenizer.vocab_size ## YOUR CODE HERE\n","meta_vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m53DcCdFV0_a","executionInfo":{"status":"ok","timestamp":1699468329297,"user_tz":-60,"elapsed":5,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"353b5bca-0160-4d2d-bbe3-58d48d768b18"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20099"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["Create our model args dict.\n","\n","Use the following as a guide: [Here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L109)"],"metadata":{"id":"V7bcNelYARmD"}},{"cell_type":"code","source":["model_args = dict(\n","    vocab_size= meta_vocab_size,\n","    n_layer= n_layer,\n","    n_head= n_head,\n","    n_embd= n_embd,\n","    dropout= dropout,\n","    bias= bias, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n","    block_size: block_size,\n",")"],"metadata":{"id":"JfIWEbanV7ZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instantiate our model with the provided `model_args`.\n","\n","These are derived from the hyper-parameters we set above."],"metadata":{"id":"2WWcbkiCAUI2"}},{"cell_type":"code","source":["if init_from == 'scratch':\n","    print(\"Initializing a new model from scratch\")\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Xly4iA0V-vF","executionInfo":{"status":"ok","timestamp":1699468830701,"user_tz":-60,"elapsed":1327,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"272e6da3-e964-45e2-c542-2556dfe1410d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing a new model from scratch\n","number of parameters: 29.55M\n"]}]},{"cell_type":"markdown","source":["There we go! If you used the default values - you should have a model with 29.55M parameters!\n","\n","Let's set our block_size to the correct size as determined in our configuration steps."],"metadata":{"id":"BpViOsxLAl6p"}},{"cell_type":"code","source":["if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size"],"metadata":{"id":"TrEawNxdWRhm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can look at our model in all its glory!"],"metadata":{"id":"eRgguPLKAuZ5"}},{"cell_type":"code","source":["model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaE3KSTnAtJs","executionInfo":{"status":"ok","timestamp":1699468855463,"user_tz":-60,"elapsed":5349,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"b896f71f-eb6a-4c73-b165-1952eaeaee1c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(20099, 516)\n","    (wpe): Embedding(1024, 516)\n","    (drop): Dropout(p=0.2, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln_1): LayerNorm()\n","        (attn): CausalSelfAttention(\n","          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n","          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n","          (attn_dropout): Dropout(p=0.2, inplace=False)\n","          (resid_dropout): Dropout(p=0.2, inplace=False)\n","        )\n","        (ln_2): LayerNorm()\n","        (mlp): MLP(\n","          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n","          (dropout): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm()\n","  )\n","  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",")"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."],"metadata":{"id":"LzoEY6gcBOSp"}},{"cell_type":"code","source":["scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"],"metadata":{"id":"BNUThRt4WT5H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."],"metadata":{"id":"6Zs5Hcf9BBUD"}},{"cell_type":"code","source":["optimizer = model.configure_optimizers(\n","    weight_decay,\n","    learning_rate,\n","    (beta1, beta2),\n","    device_type\n",")\n","\n","checkpoint = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YesGeUnoWViL","executionInfo":{"status":"ok","timestamp":1699468883193,"user_tz":-60,"elapsed":1856,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"e32353ac-28f9-493b-e422-54bb6b3252ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num decayed parameter tensors: 26, with 29,805,708 parameters\n","num non-decayed parameter tensors: 13, with 6,708 parameters\n","using fused AdamW: True\n"]}]},{"cell_type":"markdown","source":["Now we can compile our model!\n","\n","If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n","\n","Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."],"metadata":{"id":"ZF5YWJoKB4og"}},{"cell_type":"code","source":["if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0FNU0T0WXdI","executionInfo":{"status":"ok","timestamp":1699468891007,"user_tz":-60,"elapsed":1652,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"c7496934-2516-48ff-f11d-2395be5ba8e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["compiling the model... (takes a ~minute)\n"]}]},{"cell_type":"markdown","source":["We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n","\n","You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."],"metadata":{"id":"p6lRcVsZCXRO"}},{"cell_type":"code","source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"lUB5zVLVWbhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating our LR Scheduler\n","\n","Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n","\n","We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n","\n","![img](https://i.imgur.com/KoFEl0b.png)\n","\n","There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"],"metadata":{"id":"fLsOpaACDDkF"}},{"cell_type":"code","source":["def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > lr_decay_iters:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n","    return min_lr + coeff * (learning_rate - min_lr)"],"metadata":{"id":"7-mNpWBSWdHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to set some specific values in our env to allow training in Colab."],"metadata":{"id":"cqFePCZmE1Lq"}},{"cell_type":"code","source":["!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7nDL6s4YT6E","executionInfo":{"status":"ok","timestamp":1699468915790,"user_tz":-60,"elapsed":3324,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"0ee9cebb-4f00-4b99-fb0a-f99a8fea1015"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n"]}]},{"cell_type":"markdown","source":["## The Training Loop\n","\n","Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"],"metadata":{"id":"Nhqmxeo0Eg0Z"}},{"cell_type":"code","source":["X, Y = get_batch('train')\n","t0 = time.time()\n","local_iter_num = 0\n","raw_model = model\n","running_mfu = -1.0 # model flops utilization\n","\n","while True:\n","    # determine and set the learning rate for this iteration\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    # evaluate the loss on train/val sets and write checkpoints\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","\n","    # forward backward update, with optional gradient accumulation to simulate larger batch size\n","    # and using the GradScaler if data type is float16\n","    for micro_step in range(gradient_accumulation_steps):\n","        with ctx:\n","            logits, loss = model(X, Y)\n","            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n","        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n","        X, Y = get_batch('train')\n","        # backward pass, with gradient scaling if training in fp16\n","        scaler.scale(loss).backward()\n","    # clip the gradient\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    # step the optimizer and scaler if training in fp16\n","    scaler.step(optimizer)\n","    scaler.update()\n","    # flush the gradients as soon as we can, no need for this memory anymore\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # timing and logging\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        # get loss as float. note: this is a CPU-GPU sync point\n","        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5: # let the training loop settle a bit\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n","        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","    iter_num += 1\n","    local_iter_num += 1\n","\n","    # termination conditions\n","    if iter_num > max_iters:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kHbyEapRWmpc","executionInfo":{"status":"ok","timestamp":1699470482805,"user_tz":-60,"elapsed":1557623,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"01bf2e25-f9f7-4716-b406-fa86683410e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 9.9658, val loss 9.9556\n","iter 0: loss 9.9753, time 78388.11ms, mfu -100.00%\n","iter 10: loss 8.4249, time 194.61ms, mfu 2.65%\n","iter 20: loss 7.4585, time 203.13ms, mfu 2.64%\n","iter 30: loss 6.4337, time 198.93ms, mfu 2.63%\n","iter 40: loss 5.8298, time 195.83ms, mfu 2.63%\n","iter 50: loss 5.6174, time 196.41ms, mfu 2.63%\n","iter 60: loss 5.4854, time 193.86ms, mfu 2.63%\n","iter 70: loss 5.1730, time 196.32ms, mfu 2.63%\n","iter 80: loss 4.9531, time 196.80ms, mfu 2.63%\n","iter 90: loss 4.9381, time 203.15ms, mfu 2.62%\n","iter 100: loss 4.7955, time 203.85ms, mfu 2.61%\n","iter 110: loss 4.5968, time 205.98ms, mfu 2.60%\n","iter 120: loss 4.6668, time 201.13ms, mfu 2.60%\n","iter 130: loss 4.4075, time 201.27ms, mfu 2.59%\n","iter 140: loss 4.6456, time 203.20ms, mfu 2.59%\n","iter 150: loss 4.4368, time 199.91ms, mfu 2.59%\n","iter 160: loss 4.4962, time 203.29ms, mfu 2.58%\n","iter 170: loss 4.3493, time 203.52ms, mfu 2.58%\n","iter 180: loss 4.2469, time 204.14ms, mfu 2.57%\n","iter 190: loss 4.2631, time 206.15ms, mfu 2.57%\n","iter 200: loss 4.0998, time 204.20ms, mfu 2.56%\n","iter 210: loss 4.1581, time 209.50ms, mfu 2.55%\n","iter 220: loss 4.3024, time 210.02ms, mfu 2.54%\n","iter 230: loss 4.0540, time 209.77ms, mfu 2.53%\n","iter 240: loss 3.9871, time 212.12ms, mfu 2.52%\n","step 250: train loss 3.9721, val loss 4.9819\n","saving checkpoint to out\n","iter 250: loss 3.9232, time 23219.75ms, mfu 2.27%\n","iter 260: loss 4.0242, time 206.84ms, mfu 2.29%\n","iter 270: loss 3.9506, time 210.28ms, mfu 2.31%\n","iter 280: loss 3.9020, time 207.99ms, mfu 2.33%\n","iter 290: loss 3.8767, time 209.58ms, mfu 2.34%\n","iter 300: loss 3.9257, time 206.81ms, mfu 2.36%\n","iter 310: loss 3.9303, time 206.00ms, mfu 2.37%\n","iter 320: loss 3.7534, time 207.79ms, mfu 2.38%\n","iter 330: loss 3.8471, time 205.02ms, mfu 2.39%\n","iter 340: loss 3.8460, time 209.52ms, mfu 2.40%\n","iter 350: loss 3.9144, time 207.78ms, mfu 2.41%\n","iter 360: loss 3.9203, time 209.22ms, mfu 2.41%\n","iter 370: loss 3.6893, time 204.83ms, mfu 2.42%\n","iter 380: loss 3.5682, time 204.50ms, mfu 2.43%\n","iter 390: loss 3.6322, time 203.89ms, mfu 2.44%\n","iter 400: loss 3.7457, time 203.21ms, mfu 2.45%\n","iter 410: loss 3.5623, time 204.18ms, mfu 2.46%\n","iter 420: loss 3.7122, time 205.44ms, mfu 2.46%\n","iter 430: loss 3.4799, time 204.95ms, mfu 2.47%\n","iter 440: loss 3.5477, time 207.55ms, mfu 2.47%\n","iter 450: loss 3.5145, time 203.08ms, mfu 2.48%\n","iter 460: loss 3.6138, time 203.40ms, mfu 2.48%\n","iter 470: loss 3.5355, time 204.34ms, mfu 2.49%\n","iter 480: loss 3.4936, time 206.77ms, mfu 2.49%\n","iter 490: loss 3.3849, time 208.51ms, mfu 2.49%\n","step 500: train loss 3.3258, val loss 5.1020\n","saving checkpoint to out\n","iter 500: loss 3.4379, time 22856.40ms, mfu 2.24%\n","iter 510: loss 3.3724, time 207.18ms, mfu 2.26%\n","iter 520: loss 3.3266, time 207.39ms, mfu 2.29%\n","iter 530: loss 3.3999, time 207.52ms, mfu 2.31%\n","iter 540: loss 3.3857, time 208.90ms, mfu 2.32%\n","iter 550: loss 3.3867, time 207.40ms, mfu 2.34%\n","iter 560: loss 3.4623, time 209.05ms, mfu 2.35%\n","iter 570: loss 3.2084, time 206.43ms, mfu 2.37%\n","iter 580: loss 3.2879, time 206.22ms, mfu 2.38%\n","iter 590: loss 3.1613, time 207.60ms, mfu 2.39%\n","iter 600: loss 3.0921, time 206.91ms, mfu 2.40%\n","iter 610: loss 3.0170, time 209.20ms, mfu 2.41%\n","iter 620: loss 3.1359, time 206.93ms, mfu 2.41%\n","iter 630: loss 3.0522, time 208.08ms, mfu 2.42%\n","iter 640: loss 3.0412, time 208.21ms, mfu 2.43%\n","iter 650: loss 3.1344, time 208.52ms, mfu 2.43%\n","iter 660: loss 2.9797, time 204.88ms, mfu 2.44%\n","iter 670: loss 3.0382, time 209.36ms, mfu 2.44%\n","iter 680: loss 3.1638, time 205.16ms, mfu 2.45%\n","iter 690: loss 2.9386, time 206.84ms, mfu 2.45%\n","iter 700: loss 2.9481, time 209.51ms, mfu 2.45%\n","iter 710: loss 2.8505, time 204.57ms, mfu 2.46%\n","iter 720: loss 2.8864, time 205.43ms, mfu 2.47%\n","iter 730: loss 2.8033, time 204.32ms, mfu 2.47%\n","iter 740: loss 2.7483, time 206.99ms, mfu 2.47%\n","step 750: train loss 2.6692, val loss 5.4597\n","saving checkpoint to out\n","iter 750: loss 3.0242, time 22707.24ms, mfu 2.23%\n","iter 760: loss 2.9288, time 203.71ms, mfu 2.26%\n","iter 770: loss 2.7785, time 209.21ms, mfu 2.28%\n","iter 780: loss 2.8490, time 206.25ms, mfu 2.30%\n","iter 790: loss 2.7300, time 210.63ms, mfu 2.32%\n","iter 800: loss 2.7461, time 205.17ms, mfu 2.34%\n","iter 810: loss 2.8292, time 209.06ms, mfu 2.35%\n","iter 820: loss 2.6242, time 205.20ms, mfu 2.36%\n","iter 830: loss 2.8416, time 208.85ms, mfu 2.37%\n","iter 840: loss 2.8342, time 210.58ms, mfu 2.38%\n","iter 850: loss 2.3885, time 206.92ms, mfu 2.39%\n","iter 860: loss 2.5350, time 208.24ms, mfu 2.40%\n","iter 870: loss 2.6496, time 207.14ms, mfu 2.41%\n","iter 880: loss 2.5387, time 208.72ms, mfu 2.42%\n","iter 890: loss 2.3670, time 204.43ms, mfu 2.43%\n","iter 900: loss 2.2176, time 208.09ms, mfu 2.43%\n","iter 910: loss 2.4165, time 211.65ms, mfu 2.43%\n","iter 920: loss 2.2342, time 207.74ms, mfu 2.44%\n","iter 930: loss 2.3388, time 207.60ms, mfu 2.44%\n","iter 940: loss 2.2369, time 209.14ms, mfu 2.44%\n","iter 950: loss 2.1958, time 209.68ms, mfu 2.45%\n","iter 960: loss 2.3349, time 208.98ms, mfu 2.45%\n","iter 970: loss 2.3531, time 209.94ms, mfu 2.45%\n","iter 980: loss 2.2132, time 207.68ms, mfu 2.45%\n","iter 990: loss 2.2768, time 205.87ms, mfu 2.46%\n","step 1000: train loss 1.9169, val loss 5.8656\n","saving checkpoint to out\n","iter 1000: loss 2.2048, time 22837.59ms, mfu 2.21%\n","iter 1010: loss 2.3029, time 203.16ms, mfu 2.25%\n","iter 1020: loss 2.1776, time 205.08ms, mfu 2.27%\n","iter 1030: loss 2.2130, time 209.30ms, mfu 2.29%\n","iter 1040: loss 2.2961, time 209.55ms, mfu 2.31%\n","iter 1050: loss 2.0284, time 209.62ms, mfu 2.32%\n","iter 1060: loss 2.2330, time 205.81ms, mfu 2.34%\n","iter 1070: loss 2.1529, time 209.77ms, mfu 2.35%\n","iter 1080: loss 2.0458, time 207.95ms, mfu 2.37%\n","iter 1090: loss 1.8797, time 206.05ms, mfu 2.38%\n","iter 1100: loss 2.0022, time 208.79ms, mfu 2.39%\n","iter 1110: loss 1.9588, time 206.43ms, mfu 2.40%\n","iter 1120: loss 1.7837, time 208.40ms, mfu 2.41%\n","iter 1130: loss 1.9060, time 206.54ms, mfu 2.42%\n","iter 1140: loss 1.7767, time 209.48ms, mfu 2.42%\n","iter 1150: loss 1.8375, time 205.23ms, mfu 2.43%\n","iter 1160: loss 1.9225, time 206.55ms, mfu 2.44%\n","iter 1170: loss 1.9371, time 205.92ms, mfu 2.44%\n","iter 1180: loss 1.7352, time 208.25ms, mfu 2.45%\n","iter 1190: loss 1.7793, time 204.42ms, mfu 2.45%\n","iter 1200: loss 1.8804, time 207.90ms, mfu 2.46%\n","iter 1210: loss 1.8016, time 205.99ms, mfu 2.46%\n","iter 1220: loss 1.8319, time 207.92ms, mfu 2.46%\n","iter 1230: loss 1.7270, time 209.13ms, mfu 2.46%\n","iter 1240: loss 1.7333, time 208.83ms, mfu 2.46%\n","step 1250: train loss 1.3147, val loss 6.2897\n","saving checkpoint to out\n","iter 1250: loss 1.6448, time 22774.16ms, mfu 2.22%\n","iter 1260: loss 1.6030, time 201.28ms, mfu 2.25%\n","iter 1270: loss 1.6790, time 208.88ms, mfu 2.27%\n","iter 1280: loss 1.6124, time 211.25ms, mfu 2.29%\n","iter 1290: loss 1.8241, time 204.78ms, mfu 2.31%\n","iter 1300: loss 1.5443, time 211.45ms, mfu 2.33%\n","iter 1310: loss 1.6646, time 204.36ms, mfu 2.35%\n","iter 1320: loss 1.6271, time 209.13ms, mfu 2.36%\n","iter 1330: loss 1.5376, time 206.20ms, mfu 2.37%\n","iter 1340: loss 1.6679, time 209.83ms, mfu 2.38%\n","iter 1350: loss 1.4957, time 208.83ms, mfu 2.39%\n","iter 1360: loss 1.4455, time 204.75ms, mfu 2.40%\n","iter 1370: loss 1.6115, time 210.23ms, mfu 2.41%\n","iter 1380: loss 1.6022, time 205.32ms, mfu 2.42%\n","iter 1390: loss 1.4552, time 209.67ms, mfu 2.42%\n","iter 1400: loss 1.3447, time 208.46ms, mfu 2.43%\n","iter 1410: loss 1.4433, time 208.04ms, mfu 2.43%\n","iter 1420: loss 1.5515, time 208.21ms, mfu 2.44%\n","iter 1430: loss 1.4740, time 207.21ms, mfu 2.44%\n","iter 1440: loss 1.3925, time 205.91ms, mfu 2.45%\n","iter 1450: loss 1.4353, time 207.12ms, mfu 2.45%\n","iter 1460: loss 1.3974, time 207.50ms, mfu 2.45%\n","iter 1470: loss 1.4484, time 208.51ms, mfu 2.46%\n","iter 1480: loss 1.3614, time 205.07ms, mfu 2.46%\n","iter 1490: loss 1.3707, time 205.71ms, mfu 2.47%\n","step 1500: train loss 0.9114, val loss 6.8627\n","saving checkpoint to out\n","iter 1500: loss 1.3238, time 22850.39ms, mfu 2.22%\n","iter 1510: loss 1.2840, time 205.66ms, mfu 2.25%\n","iter 1520: loss 1.3112, time 205.28ms, mfu 2.28%\n","iter 1530: loss 1.1868, time 207.88ms, mfu 2.30%\n","iter 1540: loss 1.3688, time 209.25ms, mfu 2.31%\n","iter 1550: loss 1.2205, time 209.10ms, mfu 2.33%\n","iter 1560: loss 1.2544, time 206.53ms, mfu 2.35%\n","iter 1570: loss 1.1971, time 205.16ms, mfu 2.36%\n","iter 1580: loss 1.2163, time 207.83ms, mfu 2.37%\n","iter 1590: loss 1.2414, time 202.89ms, mfu 2.39%\n","iter 1600: loss 1.1905, time 207.91ms, mfu 2.40%\n","iter 1610: loss 1.0303, time 208.86ms, mfu 2.41%\n","iter 1620: loss 1.1468, time 207.54ms, mfu 2.41%\n","iter 1630: loss 1.1376, time 203.49ms, mfu 2.43%\n","iter 1640: loss 1.1172, time 205.34ms, mfu 2.43%\n","iter 1650: loss 1.1731, time 203.65ms, mfu 2.44%\n","iter 1660: loss 1.1288, time 208.66ms, mfu 2.45%\n","iter 1670: loss 1.2243, time 209.42ms, mfu 2.45%\n","iter 1680: loss 1.1411, time 204.54ms, mfu 2.46%\n","iter 1690: loss 1.1667, time 205.50ms, mfu 2.46%\n","iter 1700: loss 1.1396, time 209.04ms, mfu 2.46%\n","iter 1710: loss 1.2385, time 205.51ms, mfu 2.47%\n","iter 1720: loss 1.0062, time 207.40ms, mfu 2.47%\n","iter 1730: loss 1.1048, time 209.83ms, mfu 2.47%\n","iter 1740: loss 1.0786, time 205.08ms, mfu 2.47%\n","step 1750: train loss 0.6754, val loss 7.2846\n","saving checkpoint to out\n","iter 1750: loss 1.0289, time 22839.94ms, mfu 2.23%\n","iter 1760: loss 0.9658, time 203.21ms, mfu 2.26%\n","iter 1770: loss 1.1102, time 208.89ms, mfu 2.28%\n","iter 1780: loss 0.9908, time 211.44ms, mfu 2.29%\n","iter 1790: loss 1.0037, time 206.74ms, mfu 2.31%\n","iter 1800: loss 1.0208, time 210.20ms, mfu 2.33%\n","iter 1810: loss 1.0536, time 206.57ms, mfu 2.34%\n","iter 1820: loss 1.0186, time 210.12ms, mfu 2.36%\n","iter 1830: loss 1.0590, time 206.21ms, mfu 2.37%\n","iter 1840: loss 1.0395, time 209.87ms, mfu 2.38%\n","iter 1850: loss 0.9649, time 206.58ms, mfu 2.39%\n","iter 1860: loss 0.9342, time 207.83ms, mfu 2.40%\n","iter 1870: loss 0.9432, time 205.74ms, mfu 2.41%\n","iter 1880: loss 0.9243, time 208.60ms, mfu 2.42%\n","iter 1890: loss 0.9330, time 209.26ms, mfu 2.42%\n","iter 1900: loss 0.9432, time 207.11ms, mfu 2.43%\n","iter 1910: loss 0.9969, time 204.91ms, mfu 2.44%\n","iter 1920: loss 0.8767, time 211.93ms, mfu 2.44%\n","iter 1930: loss 0.9969, time 204.57ms, mfu 2.44%\n","iter 1940: loss 0.8973, time 206.76ms, mfu 2.45%\n","iter 1950: loss 0.8359, time 209.66ms, mfu 2.45%\n","iter 1960: loss 0.9069, time 208.86ms, mfu 2.45%\n","iter 1970: loss 0.8701, time 208.64ms, mfu 2.45%\n","iter 1980: loss 0.9612, time 202.93ms, mfu 2.46%\n","iter 1990: loss 0.8600, time 205.22ms, mfu 2.47%\n","step 2000: train loss 0.4981, val loss 7.7274\n","saving checkpoint to out\n","iter 2000: loss 0.9193, time 22794.76ms, mfu 2.22%\n","iter 2010: loss 0.8706, time 207.05ms, mfu 2.25%\n","iter 2020: loss 0.8795, time 205.62ms, mfu 2.28%\n","iter 2030: loss 0.9003, time 207.46ms, mfu 2.30%\n","iter 2040: loss 0.8306, time 205.47ms, mfu 2.32%\n","iter 2050: loss 0.8280, time 207.89ms, mfu 2.33%\n","iter 2060: loss 0.8836, time 205.68ms, mfu 2.35%\n","iter 2070: loss 0.8245, time 207.88ms, mfu 2.36%\n","iter 2080: loss 0.8389, time 205.68ms, mfu 2.38%\n","iter 2090: loss 0.7949, time 207.62ms, mfu 2.39%\n","iter 2100: loss 0.8259, time 206.22ms, mfu 2.40%\n","iter 2110: loss 0.8058, time 205.85ms, mfu 2.41%\n","iter 2120: loss 0.8597, time 208.53ms, mfu 2.42%\n","iter 2130: loss 0.7604, time 204.76ms, mfu 2.43%\n","iter 2140: loss 0.8536, time 205.34ms, mfu 2.43%\n","iter 2150: loss 0.7340, time 205.80ms, mfu 2.44%\n","iter 2160: loss 0.8473, time 211.24ms, mfu 2.44%\n","iter 2170: loss 0.7744, time 210.41ms, mfu 2.44%\n","iter 2180: loss 0.8321, time 209.41ms, mfu 2.44%\n","iter 2190: loss 0.8140, time 205.36ms, mfu 2.45%\n","iter 2200: loss 0.7583, time 203.91ms, mfu 2.46%\n","iter 2210: loss 0.7714, time 203.24ms, mfu 2.47%\n","iter 2220: loss 0.7794, time 205.17ms, mfu 2.47%\n","iter 2230: loss 0.7835, time 204.90ms, mfu 2.48%\n","iter 2240: loss 0.7536, time 208.71ms, mfu 2.47%\n","step 2250: train loss 0.3640, val loss 7.9262\n","saving checkpoint to out\n","iter 2250: loss 0.7145, time 22816.17ms, mfu 2.23%\n","iter 2260: loss 0.7134, time 203.00ms, mfu 2.26%\n","iter 2270: loss 0.7846, time 205.71ms, mfu 2.29%\n","iter 2280: loss 0.8197, time 205.69ms, mfu 2.31%\n","iter 2290: loss 0.7089, time 204.53ms, mfu 2.33%\n","iter 2300: loss 0.7135, time 207.64ms, mfu 2.34%\n","iter 2310: loss 0.6933, time 207.91ms, mfu 2.36%\n","iter 2320: loss 0.6987, time 208.89ms, mfu 2.37%\n","iter 2330: loss 0.7293, time 205.77ms, mfu 2.38%\n","iter 2340: loss 0.6930, time 207.72ms, mfu 2.39%\n","iter 2350: loss 0.7055, time 202.81ms, mfu 2.41%\n","iter 2360: loss 0.7199, time 208.44ms, mfu 2.41%\n","iter 2370: loss 0.7275, time 204.52ms, mfu 2.42%\n","iter 2380: loss 0.6636, time 208.25ms, mfu 2.43%\n","iter 2390: loss 0.6948, time 205.04ms, mfu 2.44%\n","iter 2400: loss 0.6386, time 205.42ms, mfu 2.44%\n","iter 2410: loss 0.5907, time 207.30ms, mfu 2.45%\n","iter 2420: loss 0.6529, time 208.97ms, mfu 2.45%\n","iter 2430: loss 0.6495, time 207.50ms, mfu 2.45%\n","iter 2440: loss 0.6854, time 208.77ms, mfu 2.46%\n","iter 2450: loss 0.6797, time 206.44ms, mfu 2.46%\n","iter 2460: loss 0.6215, time 208.98ms, mfu 2.46%\n","iter 2470: loss 0.6445, time 211.39ms, mfu 2.46%\n","iter 2480: loss 0.6724, time 209.04ms, mfu 2.46%\n","iter 2490: loss 0.6250, time 201.31ms, mfu 2.47%\n","step 2500: train loss 0.2902, val loss 8.1394\n","saving checkpoint to out\n","iter 2500: loss 0.6095, time 22765.51ms, mfu 2.22%\n","iter 2510: loss 0.6192, time 202.79ms, mfu 2.26%\n","iter 2520: loss 0.5832, time 207.85ms, mfu 2.28%\n","iter 2530: loss 0.5703, time 206.77ms, mfu 2.30%\n","iter 2540: loss 0.6354, time 200.91ms, mfu 2.33%\n","iter 2550: loss 0.5710, time 204.45ms, mfu 2.35%\n","iter 2560: loss 0.6373, time 205.48ms, mfu 2.36%\n","iter 2570: loss 0.6151, time 203.74ms, mfu 2.38%\n","iter 2580: loss 0.6055, time 208.75ms, mfu 2.39%\n","iter 2590: loss 0.5421, time 202.73ms, mfu 2.40%\n","iter 2600: loss 0.5961, time 205.78ms, mfu 2.41%\n","iter 2610: loss 0.5890, time 208.97ms, mfu 2.42%\n","iter 2620: loss 0.5395, time 202.74ms, mfu 2.43%\n","iter 2630: loss 0.5764, time 205.87ms, mfu 2.44%\n","iter 2640: loss 0.6638, time 208.59ms, mfu 2.44%\n","iter 2650: loss 0.5734, time 205.79ms, mfu 2.45%\n","iter 2660: loss 0.5579, time 205.06ms, mfu 2.45%\n","iter 2670: loss 0.5324, time 204.48ms, mfu 2.46%\n","iter 2680: loss 0.5482, time 203.82ms, mfu 2.47%\n","iter 2690: loss 0.5376, time 208.12ms, mfu 2.47%\n","iter 2700: loss 0.5296, time 207.98ms, mfu 2.47%\n","iter 2710: loss 0.5221, time 202.91ms, mfu 2.48%\n","iter 2720: loss 0.5517, time 206.81ms, mfu 2.48%\n","iter 2730: loss 0.5208, time 206.75ms, mfu 2.48%\n","iter 2740: loss 0.5074, time 205.86ms, mfu 2.48%\n","step 2750: train loss 0.2236, val loss 8.3927\n","saving checkpoint to out\n","iter 2750: loss 0.4683, time 22878.19ms, mfu 2.24%\n","iter 2760: loss 0.5187, time 205.28ms, mfu 2.26%\n","iter 2770: loss 0.4979, time 204.32ms, mfu 2.29%\n","iter 2780: loss 0.5061, time 206.83ms, mfu 2.31%\n","iter 2790: loss 0.4845, time 204.05ms, mfu 2.33%\n","iter 2800: loss 0.4464, time 207.32ms, mfu 2.35%\n","iter 2810: loss 0.4850, time 208.78ms, mfu 2.36%\n","iter 2820: loss 0.4654, time 205.96ms, mfu 2.37%\n","iter 2830: loss 0.4636, time 211.29ms, mfu 2.38%\n","iter 2840: loss 0.5248, time 205.57ms, mfu 2.39%\n","iter 2850: loss 0.4799, time 206.37ms, mfu 2.40%\n","iter 2860: loss 0.4856, time 202.84ms, mfu 2.42%\n","iter 2870: loss 0.5128, time 203.06ms, mfu 2.43%\n","iter 2880: loss 0.4916, time 207.71ms, mfu 2.43%\n","iter 2890: loss 0.4436, time 206.18ms, mfu 2.44%\n","iter 2900: loss 0.4800, time 200.34ms, mfu 2.45%\n","iter 2910: loss 0.4585, time 209.53ms, mfu 2.45%\n","iter 2920: loss 0.4468, time 206.30ms, mfu 2.46%\n","iter 2930: loss 0.4458, time 203.06ms, mfu 2.47%\n","iter 2940: loss 0.4548, time 206.84ms, mfu 2.47%\n","iter 2950: loss 0.4052, time 206.84ms, mfu 2.47%\n","iter 2960: loss 0.4132, time 204.56ms, mfu 2.48%\n","iter 2970: loss 0.4539, time 205.56ms, mfu 2.48%\n","iter 2980: loss 0.4335, time 209.91ms, mfu 2.48%\n","iter 2990: loss 0.4081, time 205.41ms, mfu 2.48%\n","step 3000: train loss 0.1745, val loss 8.5393\n","saving checkpoint to out\n","iter 3000: loss 0.4213, time 22846.99ms, mfu 2.23%\n","iter 3010: loss 0.4462, time 203.91ms, mfu 2.26%\n","iter 3020: loss 0.4144, time 204.68ms, mfu 2.29%\n","iter 3030: loss 0.4059, time 210.41ms, mfu 2.31%\n","iter 3040: loss 0.4254, time 209.97ms, mfu 2.32%\n","iter 3050: loss 0.4028, time 201.85ms, mfu 2.34%\n","iter 3060: loss 0.3991, time 205.69ms, mfu 2.36%\n","iter 3070: loss 0.3758, time 203.96ms, mfu 2.38%\n","iter 3080: loss 0.4249, time 209.33ms, mfu 2.39%\n","iter 3090: loss 0.4027, time 207.83ms, mfu 2.39%\n","iter 3100: loss 0.3728, time 205.12ms, mfu 2.41%\n","iter 3110: loss 0.3809, time 206.11ms, mfu 2.42%\n","iter 3120: loss 0.3832, time 206.70ms, mfu 2.42%\n","iter 3130: loss 0.4100, time 205.46ms, mfu 2.43%\n","iter 3140: loss 0.4148, time 208.45ms, mfu 2.44%\n","iter 3150: loss 0.3957, time 203.56ms, mfu 2.45%\n","iter 3160: loss 0.3656, time 204.23ms, mfu 2.45%\n","iter 3170: loss 0.3882, time 205.91ms, mfu 2.46%\n","iter 3180: loss 0.3917, time 207.25ms, mfu 2.46%\n","iter 3190: loss 0.3480, time 207.94ms, mfu 2.46%\n","iter 3200: loss 0.4025, time 202.72ms, mfu 2.47%\n","iter 3210: loss 0.3775, time 205.35ms, mfu 2.48%\n","iter 3220: loss 0.3835, time 206.99ms, mfu 2.48%\n","iter 3230: loss 0.3645, time 202.44ms, mfu 2.48%\n","iter 3240: loss 0.3623, time 204.86ms, mfu 2.49%\n","step 3250: train loss 0.1376, val loss 8.6847\n","saving checkpoint to out\n","iter 3250: loss 0.3571, time 22820.08ms, mfu 2.24%\n","iter 3260: loss 0.3394, time 203.40ms, mfu 2.27%\n","iter 3270: loss 0.3341, time 203.96ms, mfu 2.30%\n","iter 3280: loss 0.3636, time 202.56ms, mfu 2.32%\n","iter 3290: loss 0.3698, time 206.22ms, mfu 2.34%\n","iter 3300: loss 0.3637, time 204.43ms, mfu 2.36%\n","iter 3310: loss 0.3364, time 204.51ms, mfu 2.37%\n","iter 3320: loss 0.3451, time 205.13ms, mfu 2.39%\n","iter 3330: loss 0.3622, time 204.07ms, mfu 2.40%\n","iter 3340: loss 0.3237, time 206.77ms, mfu 2.41%\n","iter 3350: loss 0.3326, time 209.97ms, mfu 2.41%\n","iter 3360: loss 0.3438, time 207.69ms, mfu 2.42%\n","iter 3370: loss 0.3749, time 206.31ms, mfu 2.43%\n","iter 3380: loss 0.3318, time 205.79ms, mfu 2.44%\n","iter 3390: loss 0.3617, time 205.19ms, mfu 2.44%\n","iter 3400: loss 0.3422, time 203.65ms, mfu 2.45%\n","iter 3410: loss 0.3699, time 204.73ms, mfu 2.46%\n","iter 3420: loss 0.3391, time 203.59ms, mfu 2.47%\n","iter 3430: loss 0.3608, time 207.24ms, mfu 2.47%\n","iter 3440: loss 0.3310, time 207.23ms, mfu 2.47%\n","iter 3450: loss 0.3364, time 202.84ms, mfu 2.48%\n","iter 3460: loss 0.3521, time 203.82ms, mfu 2.48%\n","iter 3470: loss 0.3129, time 208.22ms, mfu 2.48%\n","iter 3480: loss 0.3027, time 210.70ms, mfu 2.48%\n","iter 3490: loss 0.3359, time 209.18ms, mfu 2.48%\n","step 3500: train loss 0.1155, val loss 8.9346\n","saving checkpoint to out\n","iter 3500: loss 0.3263, time 22810.16ms, mfu 2.23%\n","iter 3510: loss 0.3492, time 208.82ms, mfu 2.26%\n","iter 3520: loss 0.2997, time 205.46ms, mfu 2.28%\n","iter 3530: loss 0.2927, time 205.43ms, mfu 2.30%\n","iter 3540: loss 0.3082, time 203.34ms, mfu 2.33%\n","iter 3550: loss 0.2921, time 202.67ms, mfu 2.35%\n","iter 3560: loss 0.2996, time 205.76ms, mfu 2.36%\n","iter 3570: loss 0.2992, time 208.78ms, mfu 2.37%\n","iter 3580: loss 0.3103, time 206.84ms, mfu 2.39%\n","iter 3590: loss 0.2944, time 204.39ms, mfu 2.40%\n","iter 3600: loss 0.3058, time 205.62ms, mfu 2.41%\n","iter 3610: loss 0.3114, time 211.82ms, mfu 2.41%\n","iter 3620: loss 0.2970, time 204.03ms, mfu 2.42%\n","iter 3630: loss 0.2938, time 204.18ms, mfu 2.43%\n","iter 3640: loss 0.2786, time 209.05ms, mfu 2.44%\n","iter 3650: loss 0.2971, time 205.56ms, mfu 2.44%\n","iter 3660: loss 0.2767, time 204.15ms, mfu 2.45%\n","iter 3670: loss 0.3019, time 207.71ms, mfu 2.46%\n","iter 3680: loss 0.2655, time 207.43ms, mfu 2.46%\n","iter 3690: loss 0.2654, time 205.76ms, mfu 2.46%\n","iter 3700: loss 0.2915, time 205.08ms, mfu 2.47%\n","iter 3710: loss 0.2862, time 208.98ms, mfu 2.47%\n","iter 3720: loss 0.2681, time 206.71ms, mfu 2.47%\n","iter 3730: loss 0.2682, time 204.49ms, mfu 2.48%\n","iter 3740: loss 0.2815, time 207.32ms, mfu 2.48%\n","step 3750: train loss 0.0982, val loss 8.9541\n","saving checkpoint to out\n","iter 3750: loss 0.2997, time 22785.82ms, mfu 2.23%\n","iter 3760: loss 0.2951, time 203.19ms, mfu 2.26%\n","iter 3770: loss 0.2625, time 205.29ms, mfu 2.29%\n","iter 3780: loss 0.2764, time 203.42ms, mfu 2.31%\n","iter 3790: loss 0.2570, time 206.19ms, mfu 2.33%\n","iter 3800: loss 0.2719, time 200.36ms, mfu 2.35%\n","iter 3810: loss 0.3039, time 210.77ms, mfu 2.36%\n","iter 3820: loss 0.2549, time 204.37ms, mfu 2.38%\n","iter 3830: loss 0.2766, time 207.01ms, mfu 2.39%\n","iter 3840: loss 0.2585, time 202.87ms, mfu 2.41%\n","iter 3850: loss 0.2910, time 209.30ms, mfu 2.41%\n","iter 3860: loss 0.2569, time 202.16ms, mfu 2.43%\n","iter 3870: loss 0.2574, time 209.60ms, mfu 2.43%\n","iter 3880: loss 0.2534, time 202.68ms, mfu 2.44%\n","iter 3890: loss 0.2793, time 202.79ms, mfu 2.45%\n","iter 3900: loss 0.2539, time 205.88ms, mfu 2.46%\n","iter 3910: loss 0.2683, time 203.28ms, mfu 2.46%\n","iter 3920: loss 0.2541, time 206.19ms, mfu 2.47%\n","iter 3930: loss 0.2626, time 205.67ms, mfu 2.47%\n","iter 3940: loss 0.2596, time 203.65ms, mfu 2.48%\n","iter 3950: loss 0.2464, time 209.23ms, mfu 2.48%\n","iter 3960: loss 0.2683, time 203.02ms, mfu 2.48%\n","iter 3970: loss 0.2657, time 205.15ms, mfu 2.49%\n","iter 3980: loss 0.2413, time 208.59ms, mfu 2.48%\n","iter 3990: loss 0.2269, time 205.24ms, mfu 2.49%\n","step 4000: train loss 0.0891, val loss 9.0906\n","saving checkpoint to out\n","iter 4000: loss 0.2647, time 22834.72ms, mfu 2.24%\n","iter 4010: loss 0.2672, time 205.84ms, mfu 2.27%\n","iter 4020: loss 0.2438, time 203.92ms, mfu 2.29%\n","iter 4030: loss 0.2441, time 208.65ms, mfu 2.31%\n","iter 4040: loss 0.2372, time 204.39ms, mfu 2.33%\n","iter 4050: loss 0.2396, time 206.35ms, mfu 2.35%\n","iter 4060: loss 0.2496, time 210.66ms, mfu 2.36%\n","iter 4070: loss 0.2332, time 203.97ms, mfu 2.37%\n","iter 4080: loss 0.2354, time 208.07ms, mfu 2.39%\n","iter 4090: loss 0.2359, time 208.11ms, mfu 2.39%\n","iter 4100: loss 0.2312, time 202.67ms, mfu 2.41%\n","iter 4110: loss 0.2347, time 205.85ms, mfu 2.42%\n","iter 4120: loss 0.2313, time 204.80ms, mfu 2.43%\n","iter 4130: loss 0.2487, time 206.47ms, mfu 2.44%\n","iter 4140: loss 0.2557, time 206.10ms, mfu 2.44%\n","iter 4150: loss 0.2208, time 209.40ms, mfu 2.44%\n","iter 4160: loss 0.2425, time 206.62ms, mfu 2.45%\n","iter 4170: loss 0.2194, time 207.17ms, mfu 2.45%\n","iter 4180: loss 0.2244, time 203.91ms, mfu 2.46%\n","iter 4190: loss 0.2175, time 204.21ms, mfu 2.47%\n","iter 4200: loss 0.2180, time 204.09ms, mfu 2.47%\n","iter 4210: loss 0.2582, time 203.71ms, mfu 2.48%\n","iter 4220: loss 0.2390, time 206.74ms, mfu 2.48%\n","iter 4230: loss 0.2087, time 204.30ms, mfu 2.48%\n","iter 4240: loss 0.2459, time 203.67ms, mfu 2.49%\n","step 4250: train loss 0.0811, val loss 9.2470\n","saving checkpoint to out\n","iter 4250: loss 0.2218, time 22787.97ms, mfu 2.24%\n","iter 4260: loss 0.2504, time 202.43ms, mfu 2.27%\n","iter 4270: loss 0.2154, time 205.60ms, mfu 2.30%\n","iter 4280: loss 0.2104, time 202.54ms, mfu 2.32%\n","iter 4290: loss 0.2180, time 205.87ms, mfu 2.34%\n","iter 4300: loss 0.2249, time 207.89ms, mfu 2.35%\n","iter 4310: loss 0.2080, time 206.68ms, mfu 2.37%\n","iter 4320: loss 0.2023, time 204.32ms, mfu 2.38%\n","iter 4330: loss 0.2104, time 204.83ms, mfu 2.40%\n","iter 4340: loss 0.2364, time 208.48ms, mfu 2.40%\n","iter 4350: loss 0.2354, time 206.46ms, mfu 2.41%\n","iter 4360: loss 0.2174, time 205.88ms, mfu 2.42%\n","iter 4370: loss 0.2120, time 203.27ms, mfu 2.43%\n","iter 4380: loss 0.1878, time 206.54ms, mfu 2.44%\n","iter 4390: loss 0.2198, time 205.81ms, mfu 2.45%\n","iter 4400: loss 0.1957, time 205.96ms, mfu 2.45%\n","iter 4410: loss 0.2207, time 207.35ms, mfu 2.46%\n","iter 4420: loss 0.2084, time 208.06ms, mfu 2.46%\n","iter 4430: loss 0.2145, time 206.48ms, mfu 2.46%\n","iter 4440: loss 0.2020, time 205.61ms, mfu 2.47%\n","iter 4450: loss 0.2246, time 200.95ms, mfu 2.48%\n","iter 4460: loss 0.2038, time 205.44ms, mfu 2.48%\n","iter 4470: loss 0.2011, time 208.35ms, mfu 2.48%\n","iter 4480: loss 0.1930, time 204.15ms, mfu 2.48%\n","iter 4490: loss 0.2122, time 206.21ms, mfu 2.48%\n","step 4500: train loss 0.0759, val loss 9.2711\n","saving checkpoint to out\n","iter 4500: loss 0.2106, time 22719.78ms, mfu 2.24%\n","iter 4510: loss 0.2171, time 202.71ms, mfu 2.27%\n","iter 4520: loss 0.1903, time 208.21ms, mfu 2.29%\n","iter 4530: loss 0.1835, time 206.05ms, mfu 2.31%\n","iter 4540: loss 0.1978, time 206.22ms, mfu 2.33%\n","iter 4550: loss 0.2052, time 203.79ms, mfu 2.35%\n","iter 4560: loss 0.2211, time 204.55ms, mfu 2.37%\n","iter 4570: loss 0.2138, time 208.31ms, mfu 2.38%\n","iter 4580: loss 0.1915, time 207.15ms, mfu 2.39%\n","iter 4590: loss 0.1921, time 207.76ms, mfu 2.40%\n","iter 4600: loss 0.1973, time 204.20ms, mfu 2.41%\n","iter 4610: loss 0.2118, time 205.90ms, mfu 2.42%\n","iter 4620: loss 0.2123, time 205.51ms, mfu 2.43%\n","iter 4630: loss 0.1874, time 203.73ms, mfu 2.44%\n","iter 4640: loss 0.1890, time 205.06ms, mfu 2.45%\n","iter 4650: loss 0.2242, time 205.40ms, mfu 2.45%\n","iter 4660: loss 0.1941, time 201.72ms, mfu 2.46%\n","iter 4670: loss 0.1948, time 206.69ms, mfu 2.47%\n","iter 4680: loss 0.2029, time 208.67ms, mfu 2.47%\n","iter 4690: loss 0.2018, time 205.54ms, mfu 2.47%\n","iter 4700: loss 0.2044, time 206.84ms, mfu 2.47%\n","iter 4710: loss 0.2018, time 207.23ms, mfu 2.47%\n","iter 4720: loss 0.1887, time 202.98ms, mfu 2.48%\n","iter 4730: loss 0.2017, time 206.46ms, mfu 2.48%\n","iter 4740: loss 0.1737, time 205.29ms, mfu 2.49%\n","step 4750: train loss 0.0723, val loss 9.2769\n","saving checkpoint to out\n","iter 4750: loss 0.1955, time 22810.68ms, mfu 2.24%\n","iter 4760: loss 0.2015, time 203.01ms, mfu 2.27%\n","iter 4770: loss 0.2152, time 207.57ms, mfu 2.29%\n","iter 4780: loss 0.1885, time 206.24ms, mfu 2.31%\n","iter 4790: loss 0.1918, time 209.48ms, mfu 2.33%\n","iter 4800: loss 0.1906, time 205.15ms, mfu 2.34%\n","iter 4810: loss 0.1939, time 209.92ms, mfu 2.36%\n","iter 4820: loss 0.1768, time 202.61ms, mfu 2.37%\n","iter 4830: loss 0.2008, time 206.17ms, mfu 2.39%\n","iter 4840: loss 0.1908, time 208.03ms, mfu 2.40%\n","iter 4850: loss 0.1661, time 202.31ms, mfu 2.41%\n","iter 4860: loss 0.1818, time 204.72ms, mfu 2.42%\n","iter 4870: loss 0.1803, time 211.58ms, mfu 2.42%\n","iter 4880: loss 0.1960, time 209.17ms, mfu 2.43%\n","iter 4890: loss 0.1922, time 210.13ms, mfu 2.43%\n","iter 4900: loss 0.2277, time 202.73ms, mfu 2.44%\n","iter 4910: loss 0.1770, time 203.57ms, mfu 2.45%\n","iter 4920: loss 0.1954, time 204.77ms, mfu 2.46%\n","iter 4930: loss 0.1673, time 205.17ms, mfu 2.46%\n","iter 4940: loss 0.2161, time 206.27ms, mfu 2.47%\n","iter 4950: loss 0.1756, time 207.16ms, mfu 2.47%\n","iter 4960: loss 0.2239, time 203.61ms, mfu 2.47%\n","iter 4970: loss 0.1825, time 204.07ms, mfu 2.48%\n","iter 4980: loss 0.1645, time 207.74ms, mfu 2.48%\n","iter 4990: loss 0.1959, time 201.88ms, mfu 2.49%\n","step 5000: train loss 0.0693, val loss 9.3673\n","saving checkpoint to out\n","iter 5000: loss 0.1990, time 22762.88ms, mfu 2.24%\n"]}]},{"cell_type":"markdown","source":["## Generating Outputs with our New Model\n","\n","Now we can leverage the `sample.py` file to generate outputs from our model!"],"metadata":{"id":"L2J5JlRxFJOM"}},{"cell_type":"markdown","source":["### Generation Set Up and Model Loading"],"metadata":{"id":"eo_QP1ITFfX2"}},{"cell_type":"code","source":["import os\n","import pickle\n","from contextlib import nullcontext\n","import torch\n","import tiktoken\n","from model import GPTConfig, GPT\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = 'out' # ignored if init_from is not 'resume'\n","start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"],"metadata":{"id":"-vftqU9LheEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model\n","if init_from == 'resume':\n","    # init from a model saved in a specific directory\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQRB3j7iiNkl","executionInfo":{"status":"ok","timestamp":1699471634779,"user_tz":-60,"elapsed":1141,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"67316a53-dd6e-4534-d045-8d0ee7a7c224"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 29.55M\n"]}]},{"cell_type":"code","source":["model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)"],"metadata":{"id":"N1YAy8DriVZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc = tokenizer\n","encode = lambda s: enc.encode(s)\n","decode = lambda l: enc.decode(l)"],"metadata":{"id":"KoB-5ZuLicAT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generation!"],"metadata":{"id":"mkTQ9wo7FjYU"}},{"cell_type":"code","source":["# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n","\n","# run generation\n","with torch.no_grad():\n","    with ctx:\n","        for k in range(num_samples):\n","            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","            print(decode(y[0].tolist()))\n","            print('---------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmTcaHCjii5l","executionInfo":{"status":"ok","timestamp":1699471672116,"user_tz":-60,"elapsed":23926,"user":{"displayName":"Eduardo Muñoz Sala","userId":"13317831924226771761"}},"outputId":"1d8a329d-0c78-4df1-d11a-f95fe8598c37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","To the noble steed the camp, I give him promise his trim belonging; and from this time,\n","For what he did before Corioli, call him,\n","With all the applause and clamour of the host,\n","CAIUS MARCIUS CORIOLANUS! Bear\n","The addition nobly ever!\n","\n","All:\n","Caius Marcius Coriolanus!\n","\n","CORIOLANUS:\n","I will go wash;\n","And when my face is fair, you shall perceive\n","Whether I blush or no: howbeit, I thank you.\n","I mean to stride your steed, and at all times\n","To undercrest your good addition\n","To the fairness of my power.\n","\n","COMINIUS:\n","So, to our tent;\n","Where, ere we do repose us, we will write\n","To Rome of our success. You, Titus Lartius,\n","Must to Corioli back: send us to Corioli back: send us to Rome\n","The best, with whom we may articulate,\n","For their own good and ours.\n","\n","LARTIUS:\n","I shall, my lord.\n","\n","CORIOLANUS:\n","The gods begin to mock me. I, that now\n","Refused most princely gifts, am bound to beg\n","Of my lord general.\n","\n","COMINIUS:\n","Take't; 'tis yours. What is't?\n","\n","CORIOLANUS:\n","I sometime lay here in Corioli\n","At a poor man's house; he used me kindly:\n","He cried to me; I saw him prisoner;\n","But then Aufidius was within my view,\n","And wrath o'erwhelm'd my pity: I request you\n","To give my poor host freedom.\n","\n","COMINIUS:\n","O, well begg'd!\n","Were he the butcher of my son, he should\n","Be free as is the wind. Deliver him, Titus.\n","\n","LARTIUS:\n","Marcius, his name?\n","\n","CORIOLANUS:\n","By Jupiter! forgot.\n","I am weary; yea, my memory is tired.\n","Have we no wine here?\n","\n","COMINIUS:\n","Go we to our tent:\n","The blood upon your visage dries; 'tis time\n","It should be look'd to: come.\n","\n","AUFIDIUS:\n","The town is ta'en!\n","\n","First Soldier:\n","'Twill be deliver'd back on good condition.\n","\n","---------------\n","\n","JULIET:\n","Do thou wilt not speak again, lest faith,\n","I have my resolution with thee, do not move,\n","By and strength shall be forsworn.\n","\n","ROMEO:\n","I will not swear, ere thou swear by thy love,\n","Which is the god of my idolatry,\n","And I'll believe thee.\n","\n","JULIET:\n","If my soul I joy of this contract,\n","Ere I'll believe thee,\n","It is too rash, too unadvised, too sudden;\n","Too like the lightning, which doth cease to be\n","Ere one can say 'It lightens.' Sweet, good night!\n","This bud of love, by summer's ripening breath,\n","May prove a beauteous flower when next we meet.\n","Come to thy heart as that within my breast!\n","\n","ROMEO:\n","O, wilt thou leave me so unsatisfied?\n","\n","JULIET:\n","What satisfaction canst thou have to-night?\n","\n","ROMEO:\n","The exchange of thy love's faithful vow for mine.\n","\n","JULIET:\n","I gave thee mine before thou didst request it:\n","And yet I would it were to give again.\n","\n","ROMEO:\n","Wouldst thou withdraw it? for what purpose, love?\n","\n","JULIET:\n","But to be frank, and give it thee again.\n","And yet I wish but for the thing I have:\n","My bounty is as boundless as the sea,\n","My love as the more I give to thee,\n","My love as deep; the more I give to thee,\n","The more I have, for both are infinite.\n","I hear some noise within; dear love, adieu!\n","Anon, good nurse! Sweet Montague, be true.\n","Stay but a little, I will come again.\n","\n","ROMEO:\n","O blessed, blessed night! I am afeard.\n","Being in night, all this is but a dream,\n","Too flattering-sweet to be substantial.\n","JULIET:\n","Three words, dear Romeo, and good night indeed.\n","If that thy bent of love be honourable,\n","Thy purpose marriage, send me word to-morrow,\n","By one that I'll procure to thee,\n","Where and what time thou wilt perform the rite;\n","And all my fortunes at thy foot I'll lay\n","And\n","---------------\n","\n","ISABELLA:\n","O my lord! the matter.\n","\n","DUKE VINCENTIO:\n","It is no other: neither heaven nor man nor\n","By every syllable a faithful verity:\n","The duke comes home to-morrow; nay, dry your eyes;\n","One of our convent, and his confessor,\n","Gives me this instance: already he hath carried\n","Notice to Escalus and Angelo,\n","Who do prepare to meet him at the gates,\n","There to give up their power. If you can, pace your wisdom\n","In that good path that I would wish it go,\n","And you shall have your bosom on this wretch,\n","Grace of the duke, revenges to your heart,\n","And general honour.\n","\n","ISABELLA:\n","I am directed by you.\n","\n","DUKE VINCENTIO:\n","This letter, then, to Friar Peter give;\n","'Tis that he sent me of the duke's return:\n","Say, by this token, I desire his company\n","At Mariana's house to-night. Her cause and yours\n","I'll perfect him withal, and he shall bring you\n","Before the duke, and to the head of Angelo\n","Accuse him home and home. For my poor self,\n","I am combined by a sacred vow\n","And shall be absent. Wend you with this letter:\n","Command these fretting waters from your eyes\n","With a light heart; trust not my holy order,\n","If I pervert your course. Who's here?\n","\n","LUCIO:\n","Good even. Friar, where's the provost?\n","\n","DUKE VINCENTIO:\n","Not within, sir.\n","\n","LUCIO:\n","O pretty Isabella, I am pale at mine heart to see\n","thine eyes so red: thou must be patient. I am fain\n","to dine and sup with water and bran; I dare not for\n","my head fill my belly; one fruitful meal would set\n","me to 't. But they say the duke will be here\n","to-morrow. By my troth, Isabel, I loved thy brother:\n","if the old fantastical duke of dark corners had been\n","at home, he had lived.\n","\n","DUKE VINCENTIO:\n","Sir, the duke is marvellous little beholding to your\n","reports; but the best is, he lives not in them.\n","\n","LUCIO:\n","Friar, thou knowest not the\n","---------------\n","\n","\n","CORIOLANUS:\n","Pray you now, be your limitation; and the tribunes\n","Endue you with the people's voice: remains\n","That, in the official marks invested, you\n","Anon do meet the senate.\n","\n","CORIOLANUS:\n","Is this done?\n","\n","SICINIUS:\n","The custom of request you have discharged:\n","The people do admit you, and are summon'd\n","To meet anon, upon your approbation.\n","\n","CORIOLANUS:\n","Where? at the senate-house?\n","\n","SICINIUS:\n","There, Coriolanus.\n","\n","CORIOLANUS:\n","May I change these garments?\n","\n","SICINIUS:\n","You may, sir.\n","\n","CORIOLANUS:\n","That I'll straight do; and, knowing myself again,\n","Repair to the senate-house.\n","\n","MENENIUS:\n","I'll keep you company. Will you along?\n","\n","BRUTUS:\n","We stay here for the people.\n","\n","SICINIUS:\n","Fare you well.\n","He has it now, and by his looks methink\n","'Tis warm at's heart.\n","\n","BRUTUS:\n","With a proud heart he wore his humble weeds.\n","will you dismiss the people?\n","\n","SICINIUS:\n","How now, my masters! have you chose this man?\n","\n","First Citizen:\n","He has our voices, sir.\n","\n","BRUTUS:\n","We pray the gods he may deserve your loves.\n","\n","Second Citizen:\n","Amen, sir: to my poor unworthy notice,\n","He mock'd us when he begg'd our voices.\n","\n","Third Citizen:\n","Certainly\n","He flouted us downright.\n","\n","First Citizen:\n","No,'tis his kind of speech: he did not mock us.\n","\n","Second Citizen:\n","Not one amongst us, save yourself, but says\n","He used us scornfully: he should have show'd us\n","His marks of merit, wounds received for's country.\n","\n","SICINIUS:\n","Why, so he did, I am sure.\n","\n","Citizens:\n","No, no; no man saw 'em.\n","\n","Third Citizen:\n","He said he had wounds, which he could show\n","in private;\n","And with his hat, thus waving it in scorn,\n","'I would be consul\n","---------------\n","\n","\n","JULIET:\n","Ay, thou hadst suck'd in thy breath, and I;\n","It is no more of breath, though not the sound of breath\n","As the whereon thou art thou art thyself art thyself art thyself;\n","For, if thou art thyself art thyself art thyself.\n","\n","Nurse:\n","I would prove a jaunt have I had!\n","\n","JULIET:\n","I would thou hadst make me.\n","\n","Nurse:\n","I would thou hadst my bones, and I thy news\n","Is thy news good nurse, I have I thy news\n","Is longer than the circumstance:\n","Well, is't good nurse? answer to that;\n","\n","JULIET:\n","Well, you have made a simple choice; you know not\n","how to choose a man: Romeo! no, not he; though his\n","face be better than any man's, yet his leg excels\n","all men's; and for a hand, and a foot, and a foot,\n","though they be not to be not to be talked on, yet they are\n","past compare: he is not the flower of courtesy,\n","but, I'll warrant him, as gentle as a lamb. Go thy\n","ways, wench; serve God. What, have you dined at home?\n","\n","JULIET:\n","No, no: but all this did I know before.\n","What says he of our marriage? what of that?\n","\n","Nurse:\n","Lord, how my head aches! what a head have I!\n","It beats as it would fall in twenty pieces.\n","My back o' t' other side,--O, my back, my back!\n","Beshrew your heart for sending me about,\n","To catch my death with jaunting up and down!\n","\n","JULIET:\n","I' faith, I am sorry that thou art not well.\n","Sweet, sweet nurse, sweet nurse, tell me, what says my love?\n","\n","Nurse:\n","Your love says, like an honest gentleman, and a\n","courteous, and a kind, and a handsome, and, I\n","warrant, a virtuous,--Where is your mother?\n","\n","JULIET:\n","Where is my mother! why, she is within;\n","Where should she be? How oddly thou repliest!\n","'Your love says, like an honest gentleman,\n","Where is your mother?'\n","\n","Nurse:\n","O God\n","---------------\n","\n","First Murderer:\n","Take that kill him over the duke shall be revenged on him.\n","\n","Second Murderer:\n","What, shall we do?\n","\n","CLARENCE:\n","Relent, keeper? no murder me, and save me.\n","\n","CLARENCE:\n","And shall be murder me? What art thou, or what art thou?\n","Your eyes do menace me: why look you pale?\n","Who sent you hither? Wherefore do you come?\n","\n","Both:\n","To, to, to, to--\n","\n","CLARENCE:\n","To murder me?\n","\n","Both:\n","Ay, ay.\n","\n","CLARENCE:\n","You scarcely have the hearts to do it.\n","Wherein, my friends, have I offended you?\n","\n","First Murderer:\n","Offended us you have not, but the king.\n","\n","CLARENCE:\n","I shall be reconciled to him again.\n","\n","Second Murderer:\n","Never, my lord; therefore prepare to die.\n","\n","CLARENCE:\n","Are you call'd forth from out a world of men\n","To slay the innocent? What is my offence?\n","Where are the evidence that do accuse me?\n","What lawful quest have given their verdict up\n","Unto the frowning judge? or who pronounced\n","The bitter sentence of poor Clarence' death? or who pronounced\n","Before I be convict by course of law,\n","To threaten me with death is most unlawful.\n","I charge you, as you hope to have redemption\n","By Christ's dear blood shed for our grievous sins,\n","That you depart and lay no hands on me\n","The deed you undertake is damnable.\n","\n","First Murderer:\n","What we will do, we do, we do upon command.\n","\n","Second Murderer:\n","And he that hath commanded is the king.\n","\n","CLARENCE:\n","Errroneous vassal! the great King of kings\n","Hath in the tables of his law commanded\n","That thou shalt do no murder: and wilt thou, then, then,\n","Spurn at his edict and fulfil a man's?\n","Take heed; for he holds vengeance in his hands,\n","To hurl upon their heads that break his law.\n","\n","Second Murderer:\n","And that same vengeance doth he hurl on thee,\n","For false\n","---------------\n","\n","And he be the bishop's deer?\n","\n","CATESBY:\n","I'll stay above the seas\n","That our course will come;\n","And, the border of this horizon,\n","We'll forward towards Warwick and his mates;\n","For well I wot that Henry is no soldier.\n","Therefore, froward Clarence! how evil it beseems thee\n","To flatter Henry and forsake thy brother!\n","Yet, as we may, we'll meet both thee and Warwick.\n","Come on, brave soldiers: doubt not of the day,\n","And, that once gotten, doubt not of large pay.\n","3 KING HENRY VI\n","\n","WARWICK:\n","What counsel, lords? Edward from Belgia,\n","With hasty Germans and blunt Hollanders,\n","Hath pass'd in safety through the narrow seas,\n","And with his troops doth march amain to London;\n","And many giddy people flock to him.\n","\n","KING HENRY VI:\n","Let's levy men, and beat him back again.\n","\n","CLARENCE:\n","A little fire is quickly trodden out;\n","Which, being suffer'd, rivers cannot quench.\n","\n","WARWICK:\n","In Warwickshire I have true-hearted friends,\n","Not mutinous in war; yet bold in war;\n","Those will I muster up: and thou, son Clarence,\n","Shalt stir up in Suffolk, Norfolk, Norfolk, and in Kent,\n","The knights and gentlemen to come with thee:\n","Thou, brother Montague, in Buckingham,\n","Northampton and in Leicestershire, shalt find\n","Men well inclined to hear what thou command'st:\n","And thou, brave Oxford, wondrous well beloved,\n","In Oxfordshire shalt muster up thy friends.\n","My sovereign, with the loving citizens,\n","Like to his island girt in with the ocean,\n","Or modest Dian circled with her nymphs,\n","Shall rest in London till we come to him.\n","Fair lords, take leave and stand not to reply.\n","Farewell, my sovereign.\n","\n","KING HENRY VI:\n","Farewell, my Hector, and my Troy's true hope.\n","\n","CLARENCE:\n","In sign of truth, I kiss your highness' hand.\n","\n","KING HENRY VI:\n","Well-minded Clarence, be thou fortunate!\n","\n","MONTAGUE:\n","Comfort, my lord; and so I take my leave.\n","\n","OXFORD:\n","And thus I\n","---------------\n","\n","Thus could make them that, depart and be Edward,\n","And I guerdon'd at my desert is honour:\n","Sin my honour lost for his wrong,\n","And to repair my honour lost for him,\n","I here renounce him and return to Henry.\n","My noble queen, let former grudges pass,\n","And henceforth I am thy true servitor:\n","I will revenge his wrong to Lady Bona,\n","And replant Henry in his former state.\n","\n","QUEEN MARGARET:\n","Warwick, these words have turn'd my hate to love;\n","And I forgive and quite forget old faults,\n","And joy that thou becomest King Henry's friend.\n","\n","WARWICK:\n","So much his friend, ay, his unfeigned friend,\n","That, if King Lewis vouchsafe to furnish us\n","With some few bands of chosen soldiers,\n","I'll undertake to land them on our coast\n","And force the tyrant from his seat by war.\n","'Tis not his new-made bride shall succor him:\n","And as for Clarence, as my letters tell me,\n","He's very likely now to fall from him,\n","For matching more for wanton lust than honour,\n","Or than for strength and safety of our country.\n","\n","BONA:\n","Dear brother, how shall Bona be revenged\n","But by thy help to this distressed queen?\n","\n","QUEEN MARGARET:\n","Renowned prince, how shall poor Henry live,\n","Unless thou rescue him from foul despair?\n","\n","BONA:\n","My quarrel and this English queen's are one.\n","\n","WARWICK:\n","And mine, fair lady Bona, joins with yours.\n","\n","KING LEWIS XI:\n","And mine with hers, and thine, and Margaret's.\n","Therefore at last I firmly am resolved\n","You shall have aid.\n","\n","QUEEN MARGARET:\n","Let me give humble thanks for all at once.\n","\n","KING LEWIS XI:\n","Then, England's messenger, return in post,\n","And tell false Edward, thy supposed king,\n","That Lewis of France is sending over masquers\n","To revel it with him and his new bride:\n","Thou seest what's past, go fear thy king withal.\n","\n","BONA:\n","Tell him, in hope he'll prove a widower shortly,\n","I'll wear the willow garland for his sake.\n","\n","QUEEN MARGARET:\n","Tell\n","---------------\n","\n","LEONTES:\n","O Paulina,\n","And all truth seek to think so.\n","\n","PAULINA:\n","That\n","Shall be when your first queen's again in breath;\n","Never till then.\n","\n","Gentleman:\n","One that gives out himself Prince Florizel,\n","Son of Polixenes, with his princess, she\n","The fairest I have yet beheld, desires access\n","To your high presence.\n","\n","LEONTES:\n","What with him? he comes not\n","Like to his father's greatness: his approach,\n","So out of circumstance and sudden, tells us\n","'Tis not a visitation framed, but forced\n","By need and accident. What train?\n","\n","Gentleman:\n","But few,\n","And those but mean.\n","\n","LEONTES:\n","His princess, say you, with him?\n","\n","Gentleman:\n","Ay, the most peerless piece of earth, I think,\n","That e'er the sun shone bright on.\n","\n","PAULINA:\n","O Hermione,\n","As every present time doth boast itself\n","Above a better gone, so must thy grave\n","Give way to what's seen now! Sir, you yourself\n","Have said and writ so, but your writing now\n","Is colder than that theme, 'She had not been,\n","Nor was not to be equall'd;'--thus your verse\n","Flow'd with her beauty once: 'tis shrewdly ebb'd,\n","To say you have seen a better.\n","\n","Gentleman:\n","Pardon, madam:\n","The one I have almost forgot,--your pardon,--\n","The other, when she has obtain'd your eye,\n","Will have your tongue too. This is a creature,\n","Would she begin a sect, might quench the zeal\n","Of all professors else, make proselytes\n","Of who she but bid follow.\n","\n","PAULINA:\n","How! not women?\n","\n","Gentleman:\n","Women will love her, that she is a woman\n","More worth than any man; men, that she is\n","The rarest of all women.\n","\n","LEONTES:\n","Go, Cleomenes;\n","Yourself, assisted with your honour'd friends,\n","Bring them to our embracement. Still, 'tis strange\n","He thus should steal upon us.\n","\n","PAULINA:\n","Had our prince,\n","Jewel of children, seen this hour\n","---------------\n","\n","Not then this testimony on my accusation overweigh,\n","That you shall perceive me to your own report me,\n","And smell of my sensual race the rein:\n","Fit thy consent to my sharp appetite;\n","Lay by all nicety and prolixious blushes,\n","That banish what they sue for; redeem thy brother\n","By yielding up thy body to my will;\n","Or else he must not only die the death,\n","But thy unkindness shall his death draw out\n","To lingering sufferance. Answer me to-morrow,\n","Or, by the affection that now guides me most,\n","I'll prove a tyrant to him. As for you,\n","Say what you can, my false o'erweighs your true.\n","\n","ISABELLA:\n","To whom should I complain? Did I tell this,\n","Who would believe me? O perilous mouths,\n","That bear in them one and the self-same tongue,\n","Either of condemnation or approof;\n","Bidding the law make court'sy to their will:\n","Hooking both right and wrong to the appetite,\n","To follow as it draws! I'll to my brother:\n","Though he hath fallen by prompture of the blood,\n","Yet hath he in him such a mind of honour.\n","That, had he twenty heads to tender down\n","On twenty bloody blocks, he'ld yield them up,\n","Before his sister should her body stoop\n","To such abhorr'd pollution.\n","Then, Isabel, live chaste, and, brother, die:\n","More than our brother is our chastity.\n","I'll tell him yet of Angelo's request,\n","And fit his mind to death, for his soul's rest.\n","\n","DUKE VINCENTIO:\n","So then you hope of pardon from Lord Angelo?\n","\n","CLAUDIO:\n","The miserable have no other medicine\n","But only hope:\n","I've hope to live, and am prepared to die.\n","\n","DUKE VINCENTIO:\n","Be absolute for death; either death or life\n","Shall thereby be the sweeter. Reason thus with life:\n","If I do lose a thing\n","That none but fools would keep: a breath thou art,\n","Servile to all the skyey influences,\n","That dost this habitation, where thou keep'st,\n","Hourly afflict: merely, thou art death's fool;\n","For him thou labour'st by thy flight to shun\n","A\n","---------------\n"]}]}]}